
@misc{min_recent_2021,
	title = {Recent {Advances} in {Natural} {Language} {Processing} via {Large} {Pre}-{Trained} {Language} {Models}: {A} {Survey}},
	shorttitle = {Recent {Advances} in {Natural} {Language} {Processing} via {Large} {Pre}-{Trained} {Language} {Models}},
	url = {http://arxiv.org/abs/2111.01243},
	doi = {10.48550/arXiv.2111.01243},
	abstract = {Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heinz, Ilana and Roth, Dan},
	month = nov,
	year = {2021},
	note = {arXiv:2111.01243 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{wang_kepler_2021,
	title = {{KEPLER}: {A} {Unified} {Model} for {Knowledge} {Embedding} and {Pre}-trained {Language} {Representation}},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {{KEPLER}},
	url = {https://doi.org/10.1162/tacl_a_00360},
	doi = {10.1162/tacl_a_00360},
	abstract = {Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.},
	urldate = {2022-10-06},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
	month = mar,
	year = {2021},
	pages = {176--194},
}

@article{li_deep_2020,
	title = {Deep entity matching with pre-trained language models},
	volume = {14},
	issn = {2150-8097},
	url = {http://doi.org/10.14778/3421424.3421431},
	doi = {10.14778/3421424.3421431},
	abstract = {We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast EM as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straight-forward application of language models such as BERT, DistilBERT, or RoBERTa pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art (SOTA), by up to 29\% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for EM. Finally, Ditto adapts a SOTA technique on data augmentation for text to EM to augment the training data with (difficult) examples. This way, Ditto is forced to learn "harder" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8\%. Perhaps more surprisingly, we establish that Ditto can achieve the previous SOTA results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale EM task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5\%.},
	number = {1},
	urldate = {2022-09-15},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Yuliang and Li, Jinfeng and Suhara, Yoshihiko and Doan, AnHai and Tan, Wang-Chiew},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Computation and Language, Computer Science - Databases},
	pages = {50--60},
}

@article{koumarelas_data_2020,
	title = {Data {Preparation} for {Duplicate} {Detection}},
	volume = {12},
	issn = {1936-1955},
	url = {http://doi.org/10.1145/3377878},
	doi = {10.1145/3377878},
	abstract = {Data errors represent a major issue in most application workflows. Before any important task can take place, a certain data quality has to be guaranteed by eliminating a number of different errors that may appear in data. Typically, most of these errors are fixed with data preparation methods, such as whitespace removal. However, the particular error of duplicate records, where multiple records refer to the same entity, is usually eliminated independently with specialized techniques. Our work is the first to bring these two areas together by applying data preparation operations under a systematic approach prior to performing duplicate detection. Our process workflow can be summarized as follows: It begins with the user providing as input a sample of the gold standard, the actual dataset, and optionally some constraints to domain-specific data preparations, such as address normalization. The preparation selection operates in two consecutive phases. First, to vastly reduce the search space of ineffective data preparations, decisions are made based on the improvement or worsening of pair similarities. Second, using the remaining data preparations an iterative leave-one-out classification process removes preparations one by one and determines the redundant preparations based on the achieved area under the precision-recall curve (AUC-PR). Using this workflow, we manage to improve the results of duplicate detection up to 19\% in AUC-PR.},
	number = {3},
	urldate = {2022-09-15},
	journal = {Journal of Data and Information Quality},
	author = {Koumarelas, Ioannis and Jiang, Lan and Naumann, Felix},
	month = jun,
	year = {2020},
	keywords = {Data preparation, data wrangling, duplicate detection, record linkage, similarity measures},
	pages = {15:1--15:24},
}

@inproceedings{zhao_auto-em_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {Auto-{EM}: {End}-to-end {Fuzzy} {Entity}-{Matching} using {Pre}-trained {Deep} {Models} and {Transfer} {Learning}},
	isbn = {978-1-4503-6674-8},
	shorttitle = {Auto-{EM}},
	url = {http://doi.org/10.1145/3308558.3313578},
	doi = {10.1145/3308558.3313578},
	abstract = {Entity matching (EM), also known as entity resolution, fuzzy join, and record linkage, refers to the process of identifying records corresponding to the same real-world entities from different data sources. It is an important and long-standing problem in data integration and data mining. So far progresses have been made mainly in the form of model improvements, where models with better accuracy are developed when large amounts of training data is available. In real-world applications we find that advanced approaches can often require too many labeled examples that is expensive to obtain, which has become a key obstacle to wider adoption. We in this work take a different tack, proposing a transfer-learning approach to EM, leveraging pre-trained EM models from large-scale, production knowledge bases (KB). Specifically, for each entity-type in KB, (e.g., location, organization, people, etc.), we use rich synonymous names of known entities in the KB as training data, to pre-train type-detection and EM models for each type, using a novel hierarchical neural network architecture we develop. Given a new EM task, with little or no training data, we can either fine-tune or directly leverage pre-trained EM models, to build end-to-end, high-quality EM systems. Experiments on a variety of real EM tasks suggest that the pre-trained approach is effective and outperforms existing EM methods.1.},
	urldate = {2022-09-15},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Chen and He, Yeye},
	month = may,
	year = {2019},
	pages = {2413--2424},
}

@article{paganelli_analyzing_2022,
	title = {Analyzing how {BERT} performs entity matching},
	volume = {15},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3529337.3529356},
	doi = {10.14778/3529337.3529356},
	abstract = {State-of-the-art Entity Matching (EM) approaches rely on transformer architectures, such as BERT, for generating highly contex-tualized embeddings of terms. The embeddings are then used to predict whether pairs of entity descriptions refer to the same real-world entity. BERT-based EM models demonstrated to be effective, but act as black-boxes for the users, who have limited insight into the motivations behind their decisions. In this paper, we perform a multi-facet analysis of the components of pre-trained and fine-tuned BERT architectures applied to an EM task. The main findings resulting from our extensive experimental evaluation are (1) the fine-tuning process applied to the EM task mainly modifies the last layers of the BERT components, but in a different way on tokens belonging to descriptions of matching / non-matching entities; (2) the special structure of the EM datasets, where records are pairs of entity descriptions is recognized by BERT; (3) the pair-wise semantic similarity of tokens is not a key knowledge exploited by BERT-based EM models.},
	number = {8},
	urldate = {2022-10-07},
	journal = {Proceedings of the VLDB Endowment},
	author = {Paganelli, Matteo and Buono, Francesco Del and Baraldi, Andrea and Guerra, Francesco},
	month = apr,
	year = {2022},
	pages = {1726--1738},
}

@article{peeters_dual-objective_2021,
	title = {Dual-objective fine-tuning of {BERT} for entity matching},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3467861.3467878},
	doi = {10.14778/3467861.3467878},
	abstract = {An increasing number of data providers have adopted shared numbering schemes such as GTIN, ISBN, DUNS, or ORCID numbers for identifying entities in the respective domain. This means for data integration that shared identifiers are often available for a subset of the entity descriptions to be integrated while such identifiers are not available for others. The challenge in these settings is to learn a matcher for entity descriptions without identifiers using the entity descriptions containing identifiers as training data. The task can be approached by learning a binary classifier which distinguishes pairs of entity descriptions for the same real-world entity from descriptions of different entities. The task can also be modeled as a multi-class classification problem by learning classifiers for identifying descriptions of individual entities. We present a dual-objective training method for BERT, called JointBERT, which combines binary matching and multi-class classification, forcing the model to predict the entity identifier for each entity description in a training pair in addition to the match/non-match decision. Our evaluation across five entity matching benchmark datasets shows that dualobjective training can increase the matching performance for seen products by 1\% to 5\% F1 compared to single-objective Transformerbased methods, given that enough training data is available for both objectives. In order to gain a deeper understanding of the strengths and weaknesses of the proposed method, we compare JointBERT to several other BERT-based matching methods as well as baseline systems along a set of specific matching challenges. This evaluation shows that JointBERT, given enough training data for both objectives, outperforms the other methods on tasks involving seen products, while it underperforms for unseen products. Using a combination of LIME explanations and domain-specific word classes, we analyze the matching decisions of the different deep learning models and conclude that BERT-based models are better at focusing on relevant word classes compared to RNN-based models.},
	language = {en},
	number = {10},
	urldate = {2022-10-07},
	journal = {Proceedings of the VLDB Endowment},
	author = {Peeters, Ralph and Bizer, Christian},
	month = jun,
	year = {2021},
	pages = {1913--1921},
}

@article{li_improving_2021,
	title = {Improving the {Efficiency} and {Effectiveness} for {BERT}-based {Entity} {Resolution}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17562},
	doi = {10.1609/aaai.v35i15.17562},
	abstract = {BERT has set a new state-of-the-art performance on entity resolution (ER) task, largely owed to fine-tuning pre-trained language models and the deep pair-wise interaction. Albeit being remarkably effective, it comes with a steep increase in computational cost, as the deep-interaction requires to exhaustively compute every tuple pair to search for co-references. For ER task, it is often prohibitively expensive due to the large cardinality to be matched. To tackle this, we introduce a siamese network structure that independently encodes tuples using BERT but delays the pair-wise interaction via an enhanced alignment network. This siamese structure enables a dedicated blocking module to quickly filter out obviously dissimilar tuple pairs, and thus drastically reduces the cardinality of fine-grained matching. Further, the blocking and entity matching are integrated into a multi-task learning framework for facilitating both tasks. Extensive experiments on multiple datasets demonstrate that our model significantly outperforms state-of-the-art models (including BERT) in both efficiency and effectiveness.},
	language = {en},
	number = {15},
	urldate = {2022-10-07},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Bing and Miao, Yukai and Wang, Yaoshu and Sun, Yifang and Wang, Wei},
	month = may,
	year = {2021},
	note = {Number: 15},
	keywords = {Text Classification \& Sentiment Analysis},
	pages = {13226--13233},
}

@misc{brunner_entity_2020,
	title = {Entity {Matching} with {Transformer} {Architectures} - {A} {Step} {Forward} in {Data} {Integration}},
	url = {https://openproceedings.org/2020/conf/edbt/paper_205.pdf},
	abstract = {Transformer architectures have proven to be very effective and provide state-of-the-art results in many natural language tasks. The attention-based architecture in combination with pre-training on large amounts of text lead to the recent breakthrough and a variety of slightly different implementations. In this paper we analyze how well four of the most recent attention-based transformer architectures (BERT[6], XLNet[33], RoBERTa[17] and DistilBERT [23]) perform on the task of entity matching - a crucial part of data integration. Entity matching (EM) is the task of finding data instances that refer to the same real-world entity. It is a challenging task if the data instances consist of long textual data or if the data instances are "dirty" due to misplaced values.},
	language = {en},
	urldate = {2022-10-07},
	publisher = {OpenProceedings.org},
	author = {Brunner, Ursin and Stockinger, Kurt},
	year = {2020},
	doi = {10.5441/002/EDBT.2020.58},
	note = {Version Number: 1
Type: dataset},
	keywords = {Database Technology},
}

@inproceedings{mudgal_deep_2018,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {Deep {Learning} for {Entity} {Matching}: {A} {Design} {Space} {Exploration}},
	isbn = {978-1-4503-4703-7},
	shorttitle = {Deep {Learning} for {Entity} {Matching}},
	url = {https://doi.org/10.1145/3183713.3196926},
	doi = {10.1145/3183713.3196926},
	abstract = {Entity matching (EM) finds data instances that refer to the same real-world entity. In this paper we examine applying deep learning (DL) to EM, to understand DL's benefits and limitations. We review many DL solutions that have been developed for related matching tasks in text processing (e.g., entity linking, textual entailment, etc.). We categorize these solutions and define a space of DL solutions for EM, as embodied by four solutions with varying representational power: SIF, RNN, Attention, and Hybrid. Next, we investigate the types of EM problems for which DL can be helpful. We consider three such problem types, which match structured data instances, textual instances, and dirty instances, respectively. We empirically compare the above four DL solutions with Magellan, a state-of-the-art learning-based EM solution. The results show that DL does not outperform current solutions on structured EM, but it can significantly outperform them on textual and dirty EM. For practitioners, this suggests that they should seriously consider using DL for textual and dirty EM problems. Finally, we analyze DL's performance and discuss future research directions.},
	urldate = {2022-10-06},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Mudgal, Sidharth and Li, Han and Rekatsinas, Theodoros and Doan, AnHai and Park, Youngchoon and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay},
	month = may,
	year = {2018},
	keywords = {deep learning, entity matching, entity resolution},
	pages = {19--34},
}

@misc{kojima_large_2022,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-ﬁelds of natural language processing (NLP) and generally known as excellent few-shot learners with task-speciﬁc exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-bystep answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difﬁcult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs’ ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding “Let’s think step by step” before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, signiﬁcantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shufﬂed Objects), without any hand-crafted fewshot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with an off-the-shelf 175B parameter model. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted through simple prompting. We hope our work not only serves as the minimal strongest zeroshot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting ﬁnetuning datasets or few-shot exemplars.},
	language = {en},
	urldate = {2022-10-06},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = oct,
	year = {2022},
	note = {arXiv:2205.11916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{hou_gradual_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {Gradual {Machine} {Learning} for {Entity} {Resolution}},
	isbn = {978-1-4503-6674-8},
	url = {https://doi.org/10.1145/3308558.3314121},
	doi = {10.1145/3308558.3314121},
	abstract = {Usually considered as a classification problem, entity resolution can be very challenging on real data due to the prevalence of dirty values. The state-of-the-art solutions for ER were built on a variety of learning models (most notably deep neural networks), which require lots of accurately labeled training data. Unfortunately, high-quality labeled data usually require expensive manual work, and are therefore not readily available in many real scenarios. In this demo, we propose a novel learning paradigm for ER, called gradual machine learning, which aims to enable effective machine labeling without the requirement for manual labeling effort. It begins with some easy instances in a task, which can be automatically labeled by the machine with high accuracy, and then gradually labels more challenging instances based on iterative factor graph inference. In gradual machine learning, the hard instances in a task are gradually labeled in small stages based on the estimated evidential certainty provided by the labeled easier instances. Our extensive experiments on real data have shown that the proposed approach performs considerably better than its unsupervised alternatives, and its performance is also highly competitive compared to the state-of-the-art supervised techniques. Using ER as a test case, we demonstrate that gradual machine learning is a promising paradigm potentially applicable to other challenging classification tasks requiring extensive labeling effort. Video: https://youtu.be/99bA9aamsgk},
	urldate = {2022-10-06},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Hou, Boyi and Chen, Qun and Shen, Jiquan and Liu, Xin and Zhong, Ping and Wang, Yanyan and Chen, Zhaoqiang and Li, Zhanhuai},
	month = may,
	year = {2019},
	keywords = {entity resolution, gradual machine learning, unsupervised learning},
	pages = {3526--3530},
}

@inproceedings{wu_zeroer_2020,
	address = {Portland OR USA},
	title = {{ZeroER}: {Entity} {Resolution} using {Zero} {Labeled} {Examples}},
	isbn = {978-1-4503-6735-6},
	shorttitle = {{ZeroER}},
	url = {https://dl.acm.org/doi/10.1145/3318464.3389743},
	doi = {10.1145/3318464.3389743},
	abstract = {Entity resolution (ER) refers to the problem of matching records in one or more relations that refer to the same realworld entity. While supervised machine learning (ML) approaches achieve the state-of-the-art results, they require a large amount of labeled examples that are expensive to obtain and often times infeasible. We investigate an important problem that vexes practitioners: is it possible to design an effective algorithm for ER that requires Zero labeled examples, yet can achieve performance comparable to supervised approaches? In this paper, we answer in the affirmative through our proposed approach dubbed ZeroER. Our approach is based on a simple observation — the similarity vectors for matches should look different from that of unmatches. Operationalizing this insight requires a number of technical innovations. First, we propose a simple yet powerful generative model based on Gaussian Mixture Models for learning the match and unmatch distributions. Second, we propose an adaptive regularization technique customized for ER that ameliorates the issue of feature overfitting. Finally, we incorporate the transitivity property into the generative model in a novel way resulting in improved accuracy. On five benchmark ER datasets, we show that ZeroER greatly outperforms existing unsupervised approaches and achieves comparable performance to supervised approaches.},
	language = {en},
	urldate = {2022-10-06},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Wu, Renzhi and Chaba, Sanya and Sawlani, Saurabh and Chu, Xu and Thirumuruganathan, Saravanan},
	month = jun,
	year = {2020},
	pages = {1149--1164},
}

@article{elmagarmid_duplicate_2007,
	title = {Duplicate {Record} {Detection}: {A} {Survey}},
	volume = {19},
	issn = {1558-2191},
	shorttitle = {Duplicate {Record} {Detection}},
	doi = {10.1109/TKDE.2007.250581},
	abstract = {Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area},
	number = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Elmagarmid, Ahmed K. and Ipeirotis, Panagiotis G. and Verykios, Vassilios S.},
	month = jan,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Cleaning, Computer Society, Computer errors, Cost function, Couplings, Detection algorithms, Duplicate detection, Mirrors, Relational databases, Scalability, Uncertainty, data cleaning, data deduplication, data integration, database hardening, entity matching., entity resolution, fuzzy duplicate detection, identity uncertainty, instance identification, name matching, record linkage},
	pages = {1--16},
}

@inproceedings{liu_k-bert_2020,
	title = {K-{BERT}: {Enabling} {Language} {Representation} with {Knowledge} {Graph}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5681},
	booktitle = {The {Thirty}-{Fourth} {AAAI} {Conference} on {Artificial} {Intelligence}, {AAAI} 2020, {The} {Thirty}-{Second} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}, {IAAI} 2020, {The} {Tenth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}, {EAAI} 2020, {New} {York}, {NY}, {USA}, {February} 7-12, 2020},
	publisher = {AAAI Press},
	author = {Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Ju, Qi and Deng, Haotang and Wang, Ping},
	year = {2020},
	pages = {2901--2908},
}

@incollection{arabnia_when_2021,
	address = {Cham},
	title = {When {Entity} {Resolution} {Meets} {Deep} {Learning}, {Is} {Similarity} {Measure} {Necessary}?},
	isbn = {978-3-030-70295-3 978-3-030-70296-0},
	url = {https://link.springer.com/10.1007/978-3-030-70296-0_10},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {Advances in {Artificial} {Intelligence} and {Applied} {Cognitive} {Computing}},
	publisher = {Springer International Publishing},
	author = {Li, Xinming and Talburt, John R. and Li, Ting and Liu, Xiangwen},
	editor = {Arabnia, Hamid R. and Ferens, Ken and de la Fuente, David and Kozerenko, Elena B. and Olivas Varela, José Angel and Tinetti, Fernando G.},
	year = {2021},
	doi = {10.1007/978-3-030-70296-0_10},
	note = {Series Title: Transactions on Computational Science and Computational Intelligence},
	pages = {127--140},
}

@article{enriquez_entity_2017,
	title = {Entity reconciliation in big data sources: {A} systematic mapping study},
	volume = {80},
	issn = {09574174},
	shorttitle = {Entity reconciliation in big data sources},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417417301550},
	doi = {10.1016/j.eswa.2017.03.010},
	language = {en},
	urldate = {2022-10-05},
	journal = {Expert Systems with Applications},
	author = {Enríquez, J.G. and Domínguez-Mayo, F.J. and Escalona, M.J. and Ross, M. and Staples, G.},
	month = sep,
	year = {2017},
	pages = {14--27},
}

@article{lin_efficient_2020,
	title = {Efficient {Entity} {Resolution} on {Heterogeneous} {Records}},
	volume = {32},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/8637043/},
	doi = {10.1109/TKDE.2019.2898191},
	abstract = {Entity resolution (ER) is the problem of identifying and merging records that refer to the same real-world entity. In many scenarios, raw records are stored under heterogeneous environment. Speciﬁcally, the schemas of records may differ from each other. To leverage such records better, most existing work assume that schema matching and data exchange have been done to convert records under different schemas to those under a predeﬁned schema. However, we observe that schema matching would lose information in some cases, which could be useful or even crucial to ER.},
	language = {en},
	number = {5},
	urldate = {2022-10-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Lin, Yiming and Wang, Hongzhi and Li, Jianzhong and Gao, Hong},
	month = may,
	year = {2020},
	pages = {912--926},
}

@misc{peters_knowledge_2019,
	title = {Knowledge {Enhanced} {Contextual} {Word} {Representations}},
	url = {http://arxiv.org/abs/1909.04164},
	doi = {10.48550/arXiv.1909.04164},
	abstract = {Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert's runtime is comparable to BERT's and it scales to large KBs.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Peters, Matthew E. and Neumann, Mark and Logan IV, Robert L. and Schwartz, Roy and Joshi, Vidur and Singh, Sameer and Smith, Noah A.},
	month = oct,
	year = {2019},
	note = {arXiv:1909.04164 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhang_ernie_2019,
	title = {{ERNIE}: {Enhanced} {Language} {Representation} with {Informative} {Entities}},
	shorttitle = {{ERNIE}},
	url = {http://arxiv.org/abs/1905.07129},
	doi = {10.48550/arXiv.1905.07129},
	abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
	month = jun,
	year = {2019},
	note = {arXiv:1905.07129 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{rahm_survey_2001,
	title = {A survey of approaches to automatic schema matching},
	volume = {10},
	issn = {10668888},
	url = {http://link.springer.com/10.1007/s007780100057},
	doi = {10.1007/s007780100057},
	abstract = {Schema matching is a basic problem in many database application domains, such as data integration, Ebusiness, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has signiﬁcant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for speciﬁc application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classiﬁcation we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.},
	language = {en},
	number = {4},
	urldate = {2022-10-05},
	journal = {The VLDB Journal},
	author = {Rahm, Erhard and Bernstein, Philip A.},
	month = dec,
	year = {2001},
	pages = {334--350},
}

@inproceedings{wang_tcn_2021,
	address = {Ljubljana Slovenia},
	title = {{TCN}: {Table} {Convolutional} {Network} for {Web} {Table} {Interpretation}},
	isbn = {978-1-4503-8312-7},
	shorttitle = {{TCN}},
	url = {https://dl.acm.org/doi/10.1145/3442381.3450090},
	doi = {10.1145/3442381.3450090},
	abstract = {Information extraction from semi-structured webpages provides valuable long-tailed facts for augmenting knowledge graph. Relational Web tables are a critical component containing additional entities and attributes of rich and diverse knowledge. However, extracting knowledge from relational tables is challenging because of sparse contextual information. Existing work linearize table cells and heavily rely on modifying deep language models such as BERT which only captures related cells information in the same table. In this work, we propose a novel relational table representation learning approach considering both the intra- and inter-table contextual information. On one hand, the proposed Table Convolutional Network model employs the attention mechanism to adaptively focus on the most informative intra-table cells of the same row or column; and, on the other hand, it aggregates inter-table contextual information from various types of implicit connections between cells across different tables. Specifically, we propose three novel aggregation modules for (i) cells of the same value, (ii) cells of the same schema position, and (iii) cells linked to the same page topic. We further devise a supervised multi-task training objective for jointly predicting column type and pairwise column relation, as well as a table cell recovery objective for pre-training. Experiments on real Web table datasets demonstrate our method can outperform competitive baselines by +4.8\% of F1 for column type prediction and by +4.1\% of F1 for pairwise column relation prediction.},
	language = {en},
	urldate = {2022-09-24},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {ACM},
	author = {Wang, Daheng and Shiralkar, Prashant and Lockard, Colin and Huang, Binxuan and Dong, Xin Luna and Jiang, Meng},
	month = apr,
	year = {2021},
	pages = {4020--4032},
}

@inproceedings{wang_k-adapter_2021,
	address = {Online},
	title = {K-{Adapter}: {Infusing} {Knowledge} into {Pre}-{Trained} {Models} with {Adapters}},
	shorttitle = {K-{Adapter}},
	url = {https://aclanthology.org/2021.findings-acl.121},
	doi = {10.18653/v1/2021.findings-acl.121},
	urldate = {2022-09-19},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Ruize and Tang, Duyu and Duan, Nan and Wei, Zhongyu and Huang, Xuanjing and Ji, Jianshu and Cao, Guihong and Jiang, Daxin and Zhou, Ming},
	month = aug,
	year = {2021},
	pages = {1405--1418},
}

@misc{ayoola_refined_2022,
	title = {{ReFinED}: {An} {Efficient} {Zero}-shot-capable {Approach} to {End}-to-{End} {Entity} {Linking}},
	shorttitle = {{ReFinED}},
	url = {http://arxiv.org/abs/2207.04108},
	doi = {10.48550/arXiv.2207.04108},
	abstract = {We introduce ReFinED, an efficient end-to-end entity linking model which uses fine-grained entity types and entity descriptions to perform linking. The model performs mention detection, fine-grained entity typing, and entity disambiguation for all mentions within a document in a single forward pass, making it more than 60 times faster than competitive existing approaches. ReFinED also surpasses state-of-the-art performance on standard entity linking datasets by an average of 3.7 F1. The model is capable of generalising to large-scale knowledge bases such as Wikidata (which has 15 times more entities than Wikipedia) and of zero-shot entity linking. The combination of speed, accuracy and scale makes ReFinED an effective and cost-efficient system for extracting entities from web-scale datasets, for which the model has been successfully deployed. Our code and pre-trained models are available at https://github.com/alexa/ReFinED},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Ayoola, Tom and Tyagi, Shubhi and Fisher, Joseph and Christodoulopoulos, Christos and Pierleoni, Andrea},
	month = jul,
	year = {2022},
	note = {arXiv:2207.04108 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{shen_entity_2021,
	title = {Entity {Linking} {Meets} {Deep} {Learning}: {Techniques} and {Solutions}},
	issn = {1558-2191},
	shorttitle = {Entity {Linking} {Meets} {Deep} {Learning}},
	doi = {10.1109/TKDE.2021.3117715},
	abstract = {Entity linking (EL) is the process of linking entity mentions appearing in web text with their corresponding entities in a knowledge base. EL plays an important role in the fields of knowledge engineering and data mining, underlying a variety of downstream applications such as knowledge base population, content analysis, relation extraction, and question answering. In recent years, deep learning (DL), which has achieved tremendous success in various domains, has also been leveraged in EL methods to surpass traditional machine learning based methods and yield the state-of-the-art performance. In this survey, we present a comprehensive review and analysis of existing DL based EL methods. First of all, we propose a new taxonomy, which organizes existing DL based EL methods using three axes: embedding, feature, and algorithm. Then we systematically survey the representative EL methods along the three axes of the taxonomy. Later, we introduce ten commonly used EL data sets and give a quantitative performance analysis of DL based EL methods over these data sets. Finally, we discuss the remaining limitations of existing methods and highlight some promising future directions.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Shen, Wei and Li, Yuhan and Liu, Yinan and Han, Jiawei and Wang, Jianyong and Yuan, Xiaojie},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Coherence, Data mining, Deep learning, Entity linking, Feature extraction, Knowledge based systems, Task analysis, Taxonomy, deep learning, entity disambiguation, knowledge base},
	pages = {1--1},
}

@misc{wu_scalable_2020,
	title = {Scalable {Zero}-shot {Entity} {Linking} with {Dense} {Entity} {Retrieval}},
	url = {http://arxiv.org/abs/1911.03814},
	doi = {10.48550/arXiv.1911.03814},
	abstract = {This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
	month = sep,
	year = {2020},
	note = {arXiv:1911.03814 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{li_efficient_2020,
	address = {Online},
	title = {Efficient {One}-{Pass} {End}-to-{End} {Entity} {Linking} for {Questions}},
	url = {https://aclanthology.org/2020.emnlp-main.522},
	doi = {10.18653/v1/2020.emnlp-main.522},
	abstract = {We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7\% and +19.6\% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Belinda Z. and Min, Sewon and Iyer, Srinivasan and Mehdad, Yashar and Yih, Wen-tau},
	month = nov,
	year = {2020},
	pages = {6433--6441},
}

@article{geerts_cleaning_2020,
	title = {Cleaning data with {Llunatic}},
	volume = {29},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-019-00586-5},
	doi = {10.1007/s00778-019-00586-5},
	abstract = {Data cleaning (or data repairing) is considered a crucial problem in many database-related tasks. It consists in making a database consistent with respect to a given set of constraints. In recent years, repairing methods have been proposed for several classes of constraints. These methods, however, tend to hard-code the strategy to repair conflicting values and are specialized toward specific classes of constraints. In this paper, we develop a general chase-based repairing framework, referred to as Llunatic, in which repairs can be obtained for a large class of constraints and by using different strategies to select preferred values. The framework is based on an elegant formalization in terms of labeled instances and partially ordered preference labels. In this context, we revisit concepts such as upgrades, repairs and the chase. In Llunatic, various repairing strategies can be slotted in, without the need for changing the underlying implementation. Furthermore, Llunatic is the first data repairing system which is DBMS-based. We report experimental results that confirm its good scalability and show that various instantiations of the framework result in repairs of good quality.},
	language = {en},
	number = {4},
	urldate = {2022-03-09},
	journal = {The VLDB Journal},
	author = {Geerts, Floris and Mecca, Giansalvatore and Papotti, Paolo and Santoro, Donatello},
	month = jul,
	year = {2020},
	pages = {867--892},
}

@article{cappuzzo_local_2020,
	title = {Local {Embeddings} for {Relational} {Data} {Integration}},
	url = {http://arxiv.org/abs/1909.01120},
	doi = {10.1145/3318464.3389742},
	abstract = {Deep learning based techniques have been recently used with promising results for data integration problems. Some methods directly use pre-trained embeddings that were trained on a large corpus such as Wikipedia. However, they may not always be an appropriate choice for enterprise datasets with custom vocabulary. Other methods adapt techniques from natural language processing to obtain embeddings for the enterprise's relational data. However, this approach blindly treats a tuple as a sentence, thus losing a large amount of contextual information present in the tuple. We propose algorithms for obtaining local embeddings that are effective for data integration tasks on relational databases. We make four major contributions. First, we describe a compact graph-based representation that allows the specification of a rich set of relationships inherent in the relational world. Second, we propose how to derive sentences from such a graph that effectively "describe" the similarity across elements (tokens, attributes, rows) in the two datasets. The embeddings are learned based on such sentences. Third, we propose effective optimization to improve the quality of the learned embeddings and the performance of integration tasks. Finally, we propose a diverse collection of criteria to evaluate relational embeddings and perform an extensive set of experiments validating them against multiple baseline methods. Our experiments show that our framework, EmbDI, produces meaningful results for data integration tasks such as schema matching and entity resolution both in supervised and unsupervised settings.},
	urldate = {2022-04-15},
	journal = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
	author = {Cappuzzo, Riccardo and Papotti, Paolo and Thirumuruganathan, Saravanan},
	month = jun,
	year = {2020},
	note = {arXiv: 1909.01120},
	keywords = {Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Machine Learning},
	pages = {1335--1349},
}

@article{ilyas_machine_2021,
	title = {Machine {Learning} and {Data} {Cleaning}: {Which} {Serves} the {Other}?},
	issn = {1936-1955},
	shorttitle = {Machine {Learning} and {Data} {Cleaning}},
	url = {https://doi.org/10.1145/3506712},
	doi = {10.1145/3506712},
	urldate = {2022-03-09},
	journal = {Journal of Data and Information Quality},
	author = {Ilyas, Ihab F. and Rekatsinas, Theodoros},
	month = dec,
	year = {2021},
	note = {Just Accepted},
	keywords = {data cleaning, machine learning},
}

@incollection{christen_data_2012,
	address = {Berlin, Heidelberg},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {The {Data} {Matching} {Process}},
	isbn = {978-3-642-31164-2},
	url = {https://doi.org/10.1007/978-3-642-31164-2_2},
	abstract = {This chapter provides an overview of the data matching process, and describes the five major steps involved in this process: data pre-processing (cleaning and standardisation), indexing, comparisons, record pair classification, and evaluation (of matching quality and of the complexity of the matching process). An example of two small database tables that contain name, address, and date of birth values is used to illustrate the tasks and challenges involved in each step of the data matching process. Part II of the book will then cover each of these five steps in more detail.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Data {Matching}: {Concepts} and {Techniques} for {Record} {Linkage}, {Entity} {Resolution}, and {Duplicate} {Detection}},
	publisher = {Springer},
	author = {Christen, Peter},
	editor = {Christen, Peter},
	year = {2012},
	doi = {10.1007/978-3-642-31164-2_2},
	keywords = {Data Match, Indexing Step, Indexing Technique, Potential Match, True Match},
	pages = {23--35},
}

@inproceedings{trabelsi_dame_2022,
	address = {Virtual Event AZ USA},
	title = {{DAME}: {Domain} {Adaptation} for {Matching} {Entities}},
	isbn = {978-1-4503-9132-0},
	shorttitle = {{DAME}},
	url = {https://dl.acm.org/doi/10.1145/3488560.3498486},
	doi = {10.1145/3488560.3498486},
	abstract = {Entity matching (EM) identifies data records that refer to the same real-world entity. Despite the effort in the past years to improve the performance in EM, the existing methods still require a huge amount of labeled data in each domain during the training phase. These methods treat each domain individually, and capture the specific signals for each dataset in EM, and this leads to overfitting on just one dataset. The knowledge that is learned from one dataset is not utilized to better understand the EM task in order to make predictions on the unseen datasets with fewer labeled samples. In this paper, we propose a new domain adaptation-based method that transfers the task knowledge from multiple source domains to a target domain. Our method presents a new setting for EM where the objective is to capture the task-specific knowledge from pretraining our model using multiple source domains, then testing our model on a target domain. We study the zero-shot learning case on the target domain, and demonstrate that our method learns the EM task and transfers knowledge to the target domain. We extensively study fine-tuning our model on the target dataset from multiple domains, and demonstrate that our model generalizes better than state-of-the-art methods in EM.},
	language = {en},
	urldate = {2022-07-26},
	booktitle = {Proceedings of the {Fifteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Trabelsi, Mohamed and Heflin, Jeff and Cao, Jin},
	month = feb,
	year = {2022},
	pages = {1016--1024},
}

@article{tang_rpt_2021,
	title = {{RPT}: relational pre-trained transformer is almost all you need towards democratizing data preparation},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {{RPT}},
	url = {https://doi.org/10.14778/3457390.3457391},
	doi = {10.14778/3457390.3457391},
	abstract = {Can AI help automate human-easy but computer-hard data preparation tasks that burden data scientists, practitioners, and crowd workers? We answer this question by presenting RPT, a denoising autoencoder for tuple-to-X models ("X" could be tuple, token, label, JSON, and so on). RPT is pre-trained for a tuple-to-tuple model by corrupting the input tuple and then learning a model to reconstruct the original tuple. It adopts a Transformer-based neural translation architecture that consists of a bidirectional encoder (similar to BERT) and a left-to-right autoregressive decoder (similar to GPT), leading to a generalization of both BERT and GPT. The pre-trained RPT can already support several common data preparation tasks such as data cleaning, auto-completion and schema matching. Better still, RPT can be fine-tuned on a wide range of data preparation tasks, such as value normalization, data transformation, data annotation, etc. To complement RPT, we also discuss several appealing techniques such as collaborative training and few-shot learning for entity resolution, and few-shot learning and NLP question-answering for information extraction. In addition, we identify a series of research opportunities to advance the field of data preparation.},
	number = {8},
	urldate = {2022-08-05},
	journal = {Proceedings of the VLDB Endowment},
	author = {Tang, Nan and Fan, Ju and Li, Fangyi and Tu, Jianhong and Du, Xiaoyong and Li, Guoliang and Madden, Sam and Ouzzani, Mourad},
	month = apr,
	year = {2021},
	pages = {1254--1261},
}

@inproceedings{hulsebos_sherlock_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Sherlock: {A} {Deep} {Learning} {Approach} to {Semantic} {Data} {Type} {Detection}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Sherlock},
	url = {http://doi.org/10.1145/3292500.3330993},
	doi = {10.1145/3292500.3330993},
	abstract = {Correctly detecting the semantic type of data columns is crucial for data science tasks such as automated data cleaning, schema matching, and data discovery. Existing data preparation and analysis systems rely on dictionary lookups and regular expression matching to detect semantic types. However, these matching-based approaches often are not robust to dirty data and only detect a limited number of types. We introduce Sherlock, a multi-input deep neural network for detecting semantic types. We train Sherlock on \$686,765\$ data columns retrieved from the VizNet corpus by matching \$78\$ semantic types from DBpedia to column headers. We characterize each matched column with \$1,588\$ features describing the statistical properties, character distributions, word embeddings, and paragraph vectors of column values. Sherlock achieves a support-weighted F\$\_1\$ score of \$0.89\$, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations.},
	urldate = {2022-09-15},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Hulsebos, Madelon and Hu, Kevin and Bakker, Michiel and Zgraggen, Emanuel and Satyanarayan, Arvind and Kraska, Tim and Demiralp, Çagatay and Hidalgo, César},
	month = jul,
	year = {2019},
	keywords = {deep learning, semantic types, tabular data, type detection},
	pages = {1500--1508},
}

@article{ebraheem_distributed_2018,
	title = {Distributed representations of tuples for entity resolution},
	volume = {11},
	issn = {2150-8097},
	url = {http://doi.org/10.14778/3236187.3236198},
	doi = {10.14778/3236187.3236198},
	abstract = {Despite the efforts in 70+ years in all aspects of entity resolution (ER), there is still a high demand for democratizing ER - by reducing the heavy human involvement in labeling data, performing feature engineering, tuning parameters, and defining blocking functions. With the recent advances in deep learning, in particular distributed representations of words (a.k.a. word embeddings), we present a novel ER system, called DeepER, that achieves good accuracy, high efficiency, as well as ease-of-use (i.e., much less human efforts). We use sophisticated composition methods, namely uni- and bi-directional recurrent neural networks (RNNs) with long short term memory (LSTM) hidden units, to convert each tuple to a distributed representation (i.e., a vector), which can in turn be used to effectively capture similarities between tuples. We consider both the case where pre-trained word embeddings are available as well the case where they are not; we present ways to learn and tune the distributed representations that are customized for a specific ER task under different scenarios. We propose a locality sensitive hashing (LSH) based blocking approach that takes all attributes of a tuple into consideration and produces much smaller blocks, compared with traditional methods that consider only a few attributes. We evaluate our algorithms on multiple datasets (including benchmarks, biomedical data, as well as multi-lingual data) and the extensive experimental results show that DeepER outperforms existing solutions.},
	number = {11},
	urldate = {2022-09-15},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ebraheem, Muhammad and Thirumuruganathan, Saravanan and Joty, Shafiq and Ouzzani, Mourad and Tang, Nan},
	month = jul,
	year = {2018},
	pages = {1454--1467},
}

@inproceedings{suhara_annotating_2022,
	address = {New York, NY, USA},
	series = {{SIGMOD} '22},
	title = {Annotating {Columns} with {Pre}-trained {Language} {Models}},
	isbn = {978-1-4503-9249-5},
	url = {http://doi.org/10.1145/3514221.3517906},
	doi = {10.1145/3514221.3517906},
	abstract = {Inferring meta information about tables, such as column headers or relationships between columns, is an active research topic in data management as we find many tables are missing some of this information. In this paper, we study the problem of annotating table columns (i.e., predicting column types and the relationships between columns) using only information from the table itself. We develop a multi-task learning framework (called Doduo) based on pre-trained language models, which takes the entire table as input and predicts column types/relations using a single model. Experimental results show that Doduo establishes new state-of-the-art performance on two benchmarks for the column type prediction and column relation prediction tasks with up to 4.0\% and 11.9\% improvements, respectively. We report that Doduo can already outperform the previous state-of-the-art performance with a minimal number of tokens, only 8 tokens per column. We release a toolbox (https://github.com/megagonlabs/doduo) and confirm the effectiveness of Doduo on a real-world data science problem through a case study.},
	urldate = {2022-09-15},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Suhara, Yoshihiko and Li, Jinfeng and Li, Yuliang and Zhang, Dan and Demiralp, Çağatay and Chen, Chen and Tan, Wang-Chiew},
	month = jun,
	year = {2022},
	keywords = {language models, multi-task learning, table understanding},
	pages = {1493--1503},
}

@misc{magellandata,

title = {The Magellan Data Repository},

howpublished = {\url{https://sites.google.com/site/anhaidgroup/projects/data}},

author = {Das, Sanjib and Doan, AnHai and G. C., Paul Suganthan and Gokhale, Chaitanya and Konda, Pradap and Govind, Yash and Paulsen, Derek},

institution = {University of Wisconsin-Madison},

year = {2022},

}

