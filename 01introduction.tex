% PLMs rarely used in data cleaning, in particular, entity resolution tasks
%  challenge 1: missing domain knowledge to help prepare data;
%  challenge 2: missing semantic data types/ a better language understanding of the textual data
%  challenge 3: "too much knowledge incorporation may divert the sentence
% from its correct meaning, which is called knowledge
% noise (KN) issue." \cite{liu_k-bert_2020}
Recent studies using \emph{transformer-based Pre-trained Language Models} (PLMs) have shown their strong ability to perform various types of NLP tasks \cite{min_recent_2021}. However, few studies have discussed the application of PLMs in the domain of data cleaning~\cite{li_deep_2020,narayan_can_2022,vos2022towards}.  
Entity resolution is a common data cleaning task that aims to identify the entries referring to the same real-world entities within or across databases~\cite{christen_data_2012}. 
% \bl{Entity resolution is a common data cleaning task ...}


% Entity resolution, also known as entity matching, record linkage, or data deduplication, is a classical problem in data integration~\cite{zhao_auto-em_2019}. 
% The typical process of entity resolution contains data preparation, data indexing or blocking, and data matching. %  challenge 1: missing domain knowledge to help prepare data;

% Prior work has shown that data 
% Prior work has shown that performing data cleaning before applying ML-based models for entity resolution can improve task performance,  e.g., tokenization or stemming, for threshold-based classifiers \cite{koumarelas_data_2020}.
% \bl{This sentence needs to be re-written for clarity.}.  
% However, it still remains under-studied how data preparators can be applied to datasets from different domains or different types. \bl{it is still an open problem how data preparatory can be applied ... and different data types} For instance,



% \bl{Most ...... the same schema ...., however, in many situations, data from different sources are heterogeneous and use different schema.(heterogeneous schema}

Most existing techniques on entity resolution assume the same schema for records from different sources \cite{elmagarmid_duplicate_2007}. However, in many situations, raw records are obtained from heterogeneous sources and use different schema \cite{enriquez_entity_2017, arabnia_when_2021}. In addition, source data is often from varied domains (e.g., publications, online products, musicians) and in different formats (e.g., numerical, textual, geolocations). 
% , which adds more difficulties for entity resolution tasks 
%Christian (2012) mentioned that schema standardization of records is an essential data preparation for entity resolution task~\cite{christen_data_2012}. 
All of these increase the difficulty for practitioners to perform entity resolution tasks without prior knowledge of the domain-specific information about the data.
% Hulsebos et al \cite{hulsebos_sherlock_2019} introduced Sherlock, a multi-input deep neural network for detecting semantic data types, in which they match 78 semantic types from DBpedia to column headers.  Similarly, Doduo \cite{suhara_annotating_2022}, a multi-task learning framework that is based on
% \emph{pre-trained language models} (PLMs) can predict column types and column relations. 
Thus, we hypothesize that enhancing the external knowledge at the schema and entity level can improve entity resolution tasks.  

With transformer-based PLMs, recent studies draw increasing attention to entity resolution problems~\cite{li_deep_2020, trabelsi_dame_2022}. However, current studies show that the performance might not be ideal when simply inputting the serialized entity pairs into PLMs for classification. Ditto \cite{li_deep_2020} injects domain information: pre-defined entity types (i.e., PRODUCT and NUM), and standardizes the numerical formats to improve the performance before feeding the serialized entity pairs into PLMs.

We push this idea further by injecting more external knowledge at the schema and entity level. Knowledge injection at the schema level aims to infer the fine-grained semantic types (e.g., ALBUM, ARTIST, PUBLISHER) for each column based on data values. 
For the entity level, entity mentions are identified from WikiData and annotated in the initial text with semantic type information of the linked entities. In addition, different formats used to inject external knowledge into the initial entity pairs may vary the performance of PLMs. Thus, three prompting methods are further explored in this study: space, slash, and constrained tuning of PLMs.
% soft position encoding with the visible matrix. 

% Contributions:?
To summarize, starting from state-of-the-art method Ditto \cite{li_deep_2020}, we propose a framework for  \textbf{K}nowledge \textbf{A}ugmented \textbf{E}ntity \textbf{R}esolution (KAER):
\begin{itemize}
    \item using \textbf{C}olumn \textbf{S}emantic \textbf{T}ype (CST) inference and \textbf{E}ntity \textbf{L}inking (EL) in order to inject domain-specific information as additional signals to pre-trained language models. 
    \item leveraging three prompting methods to better augment the acquired knowledge to PLMs.
    \item analyzing the effectiveness of different combinations of knowledge injection and prompting methods on entity resolution tasks from different domains and data types.
\end{itemize}
%\vspace{-0.3cm}

% \section{Research Problems}
% In this project, we target the following research questions:
% \begin{itemize}
%     \item (RQ$_1$) How to incorporate domain knowledge into input data when using PLMs for data cleaning?
    
%     \item (RQ$_2$) To what extent does the incorporation of domain knowledge benefit downstream data cleaning tasks, such as entity resolution? 
% \end{itemize}
