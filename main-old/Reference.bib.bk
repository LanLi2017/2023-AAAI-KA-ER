@article{li2020deep,
  title={Deep entity matching with pre-trained language models},
  author={Li, Yuliang and Li, Jinfeng and Suhara, Yoshihiko and Doan, AnHai and Tan, Wang-Chiew},
  journal={arXiv preprint arXiv:2004.00584},
  year={2020}
}
@article{koumarelas2020data,
  title={Data Preparation for Duplicate Detection},
  author={Koumarelas, Ioannis and Jiang, Lan and Naumann, Felix},
  journal={Journal of Data and Information Quality (JDIQ)},
  volume={12},
  number={3},
  pages={1--24},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@inproceedings{zhao2019auto,
  title={Auto-em: End-to-end fuzzy entity-matching using pre-trained deep models and transfer learning},
  author={Zhao, Chen and He, Yeye},
  booktitle={The World Wide Web Conference},
  pages={2413--2424},
  year={2019}
}
@inproceedings{mudgal2018deep,
  title={Deep learning for entity matching: A design space exploration},
  author={Mudgal, Sidharth and Li, Han and Rekatsinas, Theodoros and Doan, AnHai and Park, Youngchoon and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay},
  booktitle={Proceedings of the 2018 International Conference on Management of Data},
  pages={19--34},
  year={2018}
}


@misc{liu_k-bert_2019,
	title = {K-{BERT}: {Enabling} {Language} {Representation} with {Knowledge} {Graph}},
	shorttitle = {K-{BERT}},
	url = {http://arxiv.org/abs/1909.07606},
	abstract = {Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by equipped with a KG without pre-training by-self because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.},
	urldate = {2022-09-01},
	publisher = {arXiv},
	author = {Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Ju, Qi and Deng, Haotang and Wang, Ping},
	month = sep,
	year = {2019},
	note = {arXiv:1909.07606 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\fangl\\Zotero\\storage\\LV2L828D\\Liu et al. - 2019 - K-BERT Enabling Language Representation with Know.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fangl\\Zotero\\storage\\5R2PHA6A\\1909.html:text/html},
}

@article{tang_rpt_2021,
	title = {{RPT}: relational pre-trained transformer is almost all you need towards democratizing data preparation},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {{RPT}},
	url = {https://doi.org/10.14778/3457390.3457391},
	doi = {10.14778/3457390.3457391},
	abstract = {Can AI help automate human-easy but computer-hard data preparation tasks that burden data scientists, practitioners, and crowd workers? We answer this question by presenting RPT, a denoising autoencoder for tuple-to-X models ("X" could be tuple, token, label, JSON, and so on). RPT is pre-trained for a tuple-to-tuple model by corrupting the input tuple and then learning a model to reconstruct the original tuple. It adopts a Transformer-based neural translation architecture that consists of a bidirectional encoder (similar to BERT) and a left-to-right autoregressive decoder (similar to GPT), leading to a generalization of both BERT and GPT. The pre-trained RPT can already support several common data preparation tasks such as data cleaning, auto-completion and schema matching. Better still, RPT can be fine-tuned on a wide range of data preparation tasks, such as value normalization, data transformation, data annotation, etc. To complement RPT, we also discuss several appealing techniques such as collaborative training and few-shot learning for entity resolution, and few-shot learning and NLP question-answering for information extraction. In addition, we identify a series of research opportunities to advance the field of data preparation.},
	number = {8},
	urldate = {2022-08-05},
	journal = {Proceedings of the VLDB Endowment},
	author = {Tang, Nan and Fan, Ju and Li, Fangyi and Tu, Jianhong and Du, Xiaoyong and Li, Guoliang and Madden, Sam and Ouzzani, Mourad},
	month = apr,
	year = {2021},
	pages = {1254--1261},
	file = {Submitted Version:C\:\\Users\\fangl\\Zotero\\storage\\GNUM83DA\\Tang et al. - 2021 - RPT relational pre-trained transformer is almost .pdf:application/pdf},
}

@inproceedings{hulsebos_sherlock_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Sherlock: {A} {Deep} {Learning} {Approach} to {Semantic} {Data} {Type} {Detection}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Sherlock},
	url = {http://doi.org/10.1145/3292500.3330993},
	doi = {10.1145/3292500.3330993},
	abstract = {Correctly detecting the semantic type of data columns is crucial for data science tasks such as automated data cleaning, schema matching, and data discovery. Existing data preparation and analysis systems rely on dictionary lookups and regular expression matching to detect semantic types. However, these matching-based approaches often are not robust to dirty data and only detect a limited number of types. We introduce Sherlock, a multi-input deep neural network for detecting semantic types. We train Sherlock on \$686,765\$ data columns retrieved from the VizNet corpus by matching \$78\$ semantic types from DBpedia to column headers. We characterize each matched column with \$1,588\$ features describing the statistical properties, character distributions, word embeddings, and paragraph vectors of column values. Sherlock achieves a support-weighted F\$\_1\$ score of \$0.89\$, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations.},
	urldate = {2022-09-15},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Hulsebos, Madelon and Hu, Kevin and Bakker, Michiel and Zgraggen, Emanuel and Satyanarayan, Arvind and Kraska, Tim and Demiralp, Çagatay and Hidalgo, César},
	month = jul,
	year = {2019},
	keywords = {deep learning, semantic types, tabular data, type detection},
	pages = {1500--1508},
	file = {Hulsebos et al_2019_Sherlock.pdf:C\:\\Users\\fangl\\Zotero\\storage\\3ZSZLTFK\\Hulsebos et al_2019_Sherlock.pdf:application/pdf},
}

@misc{hulsebos_sherlock_2019-1,
	title = {Sherlock: {A} {Deep} {Learning} {Approach} to {Semantic} {Data} {Type} {Detection}},
	shorttitle = {Sherlock},
	url = {http://arxiv.org/abs/1905.10688},
	doi = {10.48550/arXiv.1905.10688},
	abstract = {Correctly detecting the semantic type of data columns is crucial for data science tasks such as automated data cleaning, schema matching, and data discovery. Existing data preparation and analysis systems rely on dictionary lookups and regular expression matching to detect semantic types. However, these matching-based approaches often are not robust to dirty data and only detect a limited number of types. We introduce Sherlock, a multi-input deep neural network for detecting semantic types. We train Sherlock on \$686,765\$ data columns retrieved from the VizNet corpus by matching \$78\$ semantic types from DBpedia to column headers. We characterize each matched column with \$1,588\$ features describing the statistical properties, character distributions, word embeddings, and paragraph vectors of column values. Sherlock achieves a support-weighted F\$\_1\$ score of \$0.89\$, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations.},
	urldate = {2022-09-15},
	publisher = {arXiv},
	author = {Hulsebos, Madelon and Hu, Kevin and Bakker, Michiel and Zgraggen, Emanuel and Satyanarayan, Arvind and Kraska, Tim and Demiralp, Çağatay and Hidalgo, César},
	month = may,
	year = {2019},
	note = {arXiv:1905.10688 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, Statistics - Machine Learning, Computer Science - Databases},
	file = {arXiv Fulltext PDF:C\:\\Users\\fangl\\Zotero\\storage\\F6N48SZK\\Hulsebos et al. - 2019 - Sherlock A Deep Learning Approach to Semantic Dat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fangl\\Zotero\\storage\\LDSWNM3S\\1905.html:text/html},
}

@article{ebraheem_distributed_2018,
	title = {Distributed representations of tuples for entity resolution},
	volume = {11},
	issn = {2150-8097},
	url = {http://doi.org/10.14778/3236187.3236198},
	doi = {10.14778/3236187.3236198},
	abstract = {Despite the efforts in 70+ years in all aspects of entity resolution (ER), there is still a high demand for democratizing ER - by reducing the heavy human involvement in labeling data, performing feature engineering, tuning parameters, and defining blocking functions. With the recent advances in deep learning, in particular distributed representations of words (a.k.a. word embeddings), we present a novel ER system, called DeepER, that achieves good accuracy, high efficiency, as well as ease-of-use (i.e., much less human efforts). We use sophisticated composition methods, namely uni- and bi-directional recurrent neural networks (RNNs) with long short term memory (LSTM) hidden units, to convert each tuple to a distributed representation (i.e., a vector), which can in turn be used to effectively capture similarities between tuples. We consider both the case where pre-trained word embeddings are available as well the case where they are not; we present ways to learn and tune the distributed representations that are customized for a specific ER task under different scenarios. We propose a locality sensitive hashing (LSH) based blocking approach that takes all attributes of a tuple into consideration and produces much smaller blocks, compared with traditional methods that consider only a few attributes. We evaluate our algorithms on multiple datasets (including benchmarks, biomedical data, as well as multi-lingual data) and the extensive experimental results show that DeepER outperforms existing solutions.},
	number = {11},
	urldate = {2022-09-15},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ebraheem, Muhammad and Thirumuruganathan, Saravanan and Joty, Shafiq and Ouzzani, Mourad and Tang, Nan},
	month = jul,
	year = {2018},
	pages = {1454--1467},
	file = {Ebraheem et al_2018_Distributed representations of tuples for entity resolution.pdf:C\:\\Users\\fangl\\Zotero\\storage\\LNBTYKJJ\\Ebraheem et al_2018_Distributed representations of tuples for entity resolution.pdf:application/pdf},
}

@article{koumarelas_data_2020,
	title = {Data {Preparation} for {Duplicate} {Detection}},
	volume = {12},
	issn = {1936-1955},
	url = {http://doi.org/10.1145/3377878},
	doi = {10.1145/3377878},
	abstract = {Data errors represent a major issue in most application workflows. Before any important task can take place, a certain data quality has to be guaranteed by eliminating a number of different errors that may appear in data. Typically, most of these errors are fixed with data preparation methods, such as whitespace removal. However, the particular error of duplicate records, where multiple records refer to the same entity, is usually eliminated independently with specialized techniques. Our work is the first to bring these two areas together by applying data preparation operations under a systematic approach prior to performing duplicate detection. Our process workflow can be summarized as follows: It begins with the user providing as input a sample of the gold standard, the actual dataset, and optionally some constraints to domain-specific data preparations, such as address normalization. The preparation selection operates in two consecutive phases. First, to vastly reduce the search space of ineffective data preparations, decisions are made based on the improvement or worsening of pair similarities. Second, using the remaining data preparations an iterative leave-one-out classification process removes preparations one by one and determines the redundant preparations based on the achieved area under the precision-recall curve (AUC-PR). Using this workflow, we manage to improve the results of duplicate detection up to 19\% in AUC-PR.},
	number = {3},
	urldate = {2022-09-15},
	journal = {Journal of Data and Information Quality},
	author = {Koumarelas, Ioannis and Jiang, Lan and Naumann, Felix},
	month = jun,
	year = {2020},
	keywords = {Data preparation, data wrangling, duplicate detection, record linkage, similarity measures},
	pages = {15:1--15:24},
	file = {Koumarelas et al_2020_Data Preparation for Duplicate Detection.pdf:C\:\\Users\\fangl\\Zotero\\storage\\4RPIJJ56\\Koumarelas et al_2020_Data Preparation for Duplicate Detection.pdf:application/pdf},
}

@inproceedings{zhao_auto-em_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {Auto-{EM}: {End}-to-end {Fuzzy} {Entity}-{Matching} using {Pre}-trained {Deep} {Models} and {Transfer} {Learning}},
	isbn = {978-1-4503-6674-8},
	shorttitle = {Auto-{EM}},
	url = {http://doi.org/10.1145/3308558.3313578},
	doi = {10.1145/3308558.3313578},
	abstract = {Entity matching (EM), also known as entity resolution, fuzzy join, and record linkage, refers to the process of identifying records corresponding to the same real-world entities from different data sources. It is an important and long-standing problem in data integration and data mining. So far progresses have been made mainly in the form of model improvements, where models with better accuracy are developed when large amounts of training data is available. In real-world applications we find that advanced approaches can often require too many labeled examples that is expensive to obtain, which has become a key obstacle to wider adoption. We in this work take a different tack, proposing a transfer-learning approach to EM, leveraging pre-trained EM models from large-scale, production knowledge bases (KB). Specifically, for each entity-type in KB, (e.g., location, organization, people, etc.), we use rich synonymous names of known entities in the KB as training data, to pre-train type-detection and EM models for each type, using a novel hierarchical neural network architecture we develop. Given a new EM task, with little or no training data, we can either fine-tune or directly leverage pre-trained EM models, to build end-to-end, high-quality EM systems. Experiments on a variety of real EM tasks suggest that the pre-trained approach is effective and outperforms existing EM methods.1.},
	urldate = {2022-09-15},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Chen and He, Yeye},
	month = may,
	year = {2019},
	pages = {2413--2424},
	file = {Zhao_He_2019_Auto-EM.pdf:C\:\\Users\\fangl\\Zotero\\storage\\4ZFRXBJB\\Zhao_He_2019_Auto-EM.pdf:application/pdf},
}

@inproceedings{suhara_annotating_2022,
	address = {New York, NY, USA},
	series = {{SIGMOD} '22},
	title = {Annotating {Columns} with {Pre}-trained {Language} {Models}},
	isbn = {978-1-4503-9249-5},
	url = {http://doi.org/10.1145/3514221.3517906},
	doi = {10.1145/3514221.3517906},
	abstract = {Inferring meta information about tables, such as column headers or relationships between columns, is an active research topic in data management as we find many tables are missing some of this information. In this paper, we study the problem of annotating table columns (i.e., predicting column types and the relationships between columns) using only information from the table itself. We develop a multi-task learning framework (called Doduo) based on pre-trained language models, which takes the entire table as input and predicts column types/relations using a single model. Experimental results show that Doduo establishes new state-of-the-art performance on two benchmarks for the column type prediction and column relation prediction tasks with up to 4.0\% and 11.9\% improvements, respectively. We report that Doduo can already outperform the previous state-of-the-art performance with a minimal number of tokens, only 8 tokens per column. We release a toolbox (https://github.com/megagonlabs/doduo) and confirm the effectiveness of Doduo on a real-world data science problem through a case study.},
	urldate = {2022-09-15},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Suhara, Yoshihiko and Li, Jinfeng and Li, Yuliang and Zhang, Dan and Demiralp, Çağatay and Chen, Chen and Tan, Wang-Chiew},
	month = jun,
	year = {2022},
	keywords = {language models, multi-task learning, table understanding},
	pages = {1493--1503},
	file = {Suhara et al_2022_Annotating Columns with Pre-trained Language Models.pdf:C\:\\Users\\fangl\\Zotero\\storage\\HJ5QGHT3\\Suhara et al_2022_Annotating Columns with Pre-trained Language Models.pdf:application/pdf},
}

@article{li_deep_2020,
	title = {Deep entity matching with pre-trained language models},
	volume = {14},
	issn = {2150-8097},
	url = {http://doi.org/10.14778/3421424.3421431},
	doi = {10.14778/3421424.3421431},
	abstract = {We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast EM as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straight-forward application of language models such as BERT, DistilBERT, or RoBERTa pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art (SOTA), by up to 29\% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for EM. Finally, Ditto adapts a SOTA technique on data augmentation for text to EM to augment the training data with (difficult) examples. This way, Ditto is forced to learn "harder" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8\%. Perhaps more surprisingly, we establish that Ditto can achieve the previous SOTA results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale EM task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5\%.},
	number = {1},
	urldate = {2022-09-15},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Yuliang and Li, Jinfeng and Suhara, Yoshihiko and Doan, AnHai and Tan, Wang-Chiew},
	month = sep,
	year = {2020},
	pages = {50--60},
	file = {Li et al_2020_Deep entity matching with pre-trained language models.pdf:C\:\\Users\\fangl\\Zotero\\storage\\38SAJWUM\\Li et al_2020_Deep entity matching with pre-trained language models.pdf:application/pdf},
}

@inproceedings{trabelsi_dame_2022,
	address = {Virtual Event AZ USA},
	title = {{DAME}: {Domain} {Adaptation} for {Matching} {Entities}},
	isbn = {978-1-4503-9132-0},
	shorttitle = {{DAME}},
	url = {https://dl.acm.org/doi/10.1145/3488560.3498486},
	doi = {10.1145/3488560.3498486},
	language = {en},
	urldate = {2022-07-26},
	booktitle = {Proceedings of the {Fifteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Trabelsi, Mohamed and Heflin, Jeff and Cao, Jin},
	month = feb,
	year = {2022},
	pages = {1016--1024},
	file = {Trabelsi et al. - 2022 - DAME Domain Adaptation for Matching Entities.pdf:C\:\\Users\\fangl\\Zotero\\storage\\SW2Z4SDA\\Trabelsi et al. - 2022 - DAME Domain Adaptation for Matching Entities.pdf:application/pdf},
}

@incollection{christen_data_2012,
	address = {Berlin, Heidelberg},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {The {Data} {Matching} {Process}},
	isbn = {978-3-642-31164-2},
	url = {https://doi.org/10.1007/978-3-642-31164-2_2},
	abstract = {This chapter provides an overview of the data matching process, and describes the five major steps involved in this process: data pre-processing (cleaning and standardisation), indexing, comparisons, record pair classification, and evaluation (of matching quality and of the complexity of the matching process). An example of two small database tables that contain name, address, and date of birth values is used to illustrate the tasks and challenges involved in each step of the data matching process. Part II of the book will then cover each of these five steps in more detail.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Data {Matching}: {Concepts} and {Techniques} for {Record} {Linkage}, {Entity} {Resolution}, and {Duplicate} {Detection}},
	publisher = {Springer},
	author = {Christen, Peter},
	editor = {Christen, Peter},
	year = {2012},
	doi = {10.1007/978-3-642-31164-2_2},
	keywords = {Data Match, Indexing Step, Indexing Technique, Potential Match, True Match},
	pages = {23--35},
}