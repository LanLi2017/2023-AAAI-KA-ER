
Entity resolution has been an essential and well-studied task in data cleaning research for decades. Applying traditional methods of matching entities with long sequences of textual data is challenging and expensive. Existing work has discussed the feasibility of utilizing pre-trained language models to perform entity resolution and achieved promising results. Whilst few works have discussed injecting domain knowledge to improve the performance of pre-trained language models on entity resolution tasks. 

In this study, we propose a novel framework named \textit{KAER} for augmenting external knowledge into pre-trained language models for entity resolution. We discussed the results of utilizing different knowledge augmentation and prompting methods to improve entity resolution performance, and we assume a deeper language understanding of the entity resolution problems. 

Our model achieves better results than Ditto, the existing state-of-the-art entity resolution method. In particular, 1) \textit{KAER} performs much more robustly and archives better results on dirty data, 2) with more knowledge injection, \textit{KAER} outperforms the existing baseline model on the textual dataset and dataset from the product domain. 

