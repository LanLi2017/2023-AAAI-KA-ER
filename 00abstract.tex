
Entity resolution has been an essential and well-studied task in data cleaning research for decades. Applying traditional methods of matching entities with long sequences of textual data is challenging and expensive. Existing work has discussed the feasibility of utilizing pre-trained language models to perform entity resolution and achieved promising results. On the other hand, few works have discussed injecting domain knowledge to improve the performance of pre-trained language models on entity resolution tasks. 
In this study, we propose \textbf{K}nowledge \textbf{A}ugmented \textbf{E}ntity \textbf{R}esolution (\textit{KAER}), a novel framework named for augmenting pre-trained language models with external knowledge for entity resolution. We discuss the results of utilizing different knowledge augmentation and prompting methods to improve entity resolution performance. % and we achieve a deeper language understanding of the entity resolution problems. 
Our model improves on Ditto, the existing state-of-the-art entity resolution method. In particular, 1) \textit{KAER} performs much more robustly and achieves better results on dirty data, and 2) with more knowledge injection, \textit{KAER} outperforms the existing baseline models on the textual dataset and dataset from the product domain. 

