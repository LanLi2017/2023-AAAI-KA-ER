


\subsection{Notation of Entity Resolution Task}
The formulation of the entity resolution task in this paper is based on the current work by~\citet{li_deep_2020}. The input of the entity resolution task consists of a set $M \subseteq D_1 \times D_2$, where $D_1$ and $D_2$ are two sets of data entry collections that contain duplicated entries.  
For each data entry, $e \in {(attr_i, val_i)}_{1 \leq i \leq N}$ where $N$ is the number of attributes, and $(e_1, e_2) \in M$. The task discussed in this paper focuses on: for each data entry pair $(e_1, e_2) \in M$, determine whether $e_1$ and $e_2$ refer to the same data entity.
% \yiren{TODO}

\subsection{Model Overview}
This study introduces a new framework for entity resolution: KAER (See Figure~\ref{fig:framework}). 
% \bl{in abstract + introduction}
% KAER is based on a transformer; 
% Three prompting types
KAER uses PLMs for entity resolution and contains three modules for knowledge augmentation: a) column semantic type augmentation, b) entity semantic type augmentation, and c) prompting. 
The following sections describe each module.


\subsubsection{Pre-trained language model for entity resolution.}
Following the work by~\citet{li_deep_2020}, KAER uses RoBERTa as the backbone model. For each data entry pair, $(e_1, e_2)$, 
 the text context of column names and values of $e_1$ and $e_2$ are serialized and concatenated as the input for PLMs. The [CLS] token position is used to classify whether $e_1$ and $e_2$ refer to the same entity. 
The loss for optimizing the classification objective is:
\begin{equation}
    \ell = - log\ p(y | s(e_1, e_2))
\end{equation}

where y denotes whether $e_1$ and $e_2$ refer to the same entity, and $s(\cdot,\cdot)$ denotes the serialization and transformation of entity pairs with knowledge injection and prompting methods. 
\begin{equation}
\Scale[0.8]{
    s(e_i, e_j) ::=  \text{[CLS]}\ serialize(e_i)\ \text{[SEP]}\ serialize(e_j)\ \text{[SEP]}} 
\end{equation}
where $serialize(\cdot)$ serializes each data entry.
\begin{equation}\Scale[0.8]{
\begin{split}
serialize(e_i) ::= & \text{[COL]}\ f(attr_1,pt)\ \text{[VAL]}\ g(val_1,pt)\ ...\\
            & \text{[COL]}\ f(attr_N,pt)\ \text{[VAL]}\ g(val_N,pt)   
\end{split}}
\end{equation}
where $f(attr_i, pt)$ denotes the semantic column type injection with prompting method $pt$, and $g(val_i, pt)$ denotes the entity linking injection with prompting method $pt$. The following sections describe  column semantic type injection, entity linking, and three prompting methods.

% \subsubsection{Knowledge Augmentation}

\subsubsection{Column semantic type injection.}
Column type prediction aims to predict the semantic type of each column by considering the intra-column context, so that fine-grained semantic information can be augmented to the framework. For example, ``name'' as a column name can provide more semantic information if injected with the semantic type ``song''. Here, KAER uses Sherlock~\cite{hulsebos_sherlock_2019} for column semantic type prediction. %of which the output is $f(attr_i,p) = p(attr_i, t)$, where t is the predicted semantic type.

% two methods for column type prediction: Sherlock \cite{hulsebos_sherlock_2019} and Doduo \cite{suhara_annotating_2022}. 

\textit{Sherlock} is a multi-input deep neural network trained on 686,765 data columns retrieved from the VizNet corpus by matching 78 semantic data types from DBpedia to column headers \cite{hulsebos_sherlock_2019}. Four categories of features are generated by Sherlock and fed into the neural network, including global statistics, aggregated character distributions, pre-trained word embeddings, and self-trained paragraph vectors. They train subnetworks for aggregated character distributions, pre-trained word embeddings, and paragraph vectors. Then, they concatenate the weights of the three output layers with global statistics features to form the input of Sherlock.

% \textit{Doduo} is a multi-task learning framework based on PLMs~\cite{suhara_annotating_2022}. Doduo serializes the entire table into a sequence of tokens to make it fit for Transformer-based architecture. Consider a table T consisting of a set of attributes, i.e., $T= (c_1, c_2,... c_n)$, and the list of data values stored at the column $c_i$ is denoted as $val(T.c_i)$. For each value v, it can be of string type and split into a sequence of input tokens to PLMs. The serialization strategy of Doduo is simply concatenating column values to make a sequence of tokens and feed that sequence as input to the model \cite{suhara_annotating_2022}. 

% \begin{equation}
%     serialize_{single}(c_i) ::= [CLS]\ v_1\ ...\ v_m\ [SEP], \\ 
% \end{equation}

% where [CLS] and [SEP] are special tokens to mark the beginning and end of a sequence, and ${v_1 ... v_m} \in val(T.c_i)$.

\begin{table*}[!ht]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cccc|c}
\hline
\multicolumn{1}{l|}{}                  & \multicolumn{2}{c|}{\textbf{Dirty}} & \multicolumn{4}{c|}{\textbf{Structured}}                      & \textbf{Textual} \\ \hline
\multicolumn{1}{c|}{\textbf{Injection}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}DBLP\\ -\\ GoogleScholar\end{tabular}} &
  \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}iTunes\\ -\\ Amazon\end{tabular}}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}iTunes\\ -\\ Amazon\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Amazon\\ -\\ Google\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}DBLP\\ -\\ ACM\end{tabular}} &
  \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}DBLP\\ -\\ GoogleScholar\end{tabular}}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Abt\\ -\\ Buy\end{tabular}} \\ \hline
RoBERTa                            & 95.8            & 66.7              & 85.2          & 73.2          & \textbf{98.8} & 95.9          & 88.4             \\
Ditto    & 95.2            & 77.1              & 53.8          & 76.7          & 98.6          & 95.9          & 88.9             \\ \hline
KAER (RoBERTa + CST)             & 95.8            & \textbf{90.2}     & \textbf{92.6} & 67.3          & \textbf{98.8} & 94.3          & 89.2             \\
KAER (RoBERTa + CST + /)          & 95.7            & 90.0                & 88.9          & 70.0            & 98.7          & 95.5          & \textbf{91.7}    \\
KAER (RoBERTa + CST + APE)     & 95.3            & 66.7              & 66.7          & \textbf{77.9} & 97.5          & 95.7          & 77.2             \\
KAER (RoBERTa + EL)                      & 95.6            & 87.3              & 72.4          & 73.5          & 98.5          & 95.5          & 89.1             \\
KAER (RoBERTa + EL + /)                   & 95.5            & 80.8              & 84.6          & 73.1          & 98.4          & \textbf{96.2} & 88.2             \\
KAER (RoBERTa + EL + APE)             & 95.2            & 80.9              & 69.7          & 76.2          & 97.1          & 95.3          & 74.8             \\
KAER (RoBERTa + CST + EL)       & 95.7            & 69.7              & 73.5          & 71.7          & 98.4          & \textbf{96.2} & 89.7             \\
KAER (RoBERTa + CST + EL + /)    & \textbf{96.0}     & 75.4              & 61.5          & 75.5          & 98.3          & 95.2          & 89.8             \\
KAER (RoBERTa + CST + EL + APE) & 94.8            & 66.7              & 70.0          & 73.5          & 96.5          & 94.7          & 66.8             \\ \hline
\end{tabular}
}
\vspace{-0.2cm}
\caption{Experimental results using different knowledge injection methods. All reported results are F1 scores. The two baseline models include RoBERTa without domain knowledge injection (in row 1) and Ditto with RoBERTa and its general domain knowledge injection methods (in row 2). 
% representing the baseline model, Ditto without knowledge injection, 
Subsequent rows represent the various knowledge injection and prompting methods combined with RoBERTa. ``+CST'' indicates \textbf{C}olumn \textbf{S}emantic \textbf{T}ype injection.``+EL'' indicates knowledge injection with \textbf{E}ntity \textbf{L}inking. ``+/'' represents prompting with a slash. ``+APE'' represents prompting with \textbf{A}dditional \textbf{P}osition \textbf{E}ncoding, i.e., soft positions and visible matrix.}
\label{tab:injection_results}
\vspace{-0.5cm}
\end{table*}

% \begin{table*}[!ht]
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{lccccccc}
% \hline
% \multicolumn{1}{l|}{}                  & \multicolumn{2}{c|}{\textbf{Dirty}} & \multicolumn{4}{c|}{\textbf{Structured}}                      & \textbf{Textual} \\ \hline
% \multicolumn{1}{c|}{\textbf{Injection}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}DBLP\\ -\\ GoogleScholar\end{tabular}} &
%   \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}iTunes\\ -\\ Amazon\end{tabular}}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}iTunes\\ -\\ Amazon\end{tabular}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}Amazon\\ -\\ Google\end{tabular}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}DBLP\\ -\\ ACM\end{tabular}} &
%   \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}DBLP\\ -\\ GoogleScholar\end{tabular}}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}Abt\\ -\\ Buy\end{tabular}} \\ \hline
% \textbf{Ditto} (baseline)                         & 95.8             & 66.7             & 85.2          & 73.2          & \textbf{98.8}          & 95.9          & 88.4             \\
% \textbf{Ditto + Sherlock}              & 95.8             & \textbf{90.2}    & \textbf{92.6} & 67.3          & \textbf{98.8}          & 94.3          & 89.2             \\
% \textbf{Ditto + Sherlock + /}          & 95.7             & 90.0             & 88.9          & 70.0          & 98.7          & 95.5          & \textbf{91.7}             \\
% \textbf{Ditto + Sherlock + KBert}      & 95.3             & 66.7             & 66.7          & \textbf{77.9}          & 97.5          & 95.7          & 77.2             \\
% % \textbf{Ditto + Doduo}                 & 94.8             & 88.5             & 66.7          & 72.5          & 98.8          & 95.6          & 88.6             \\
% % \textbf{Ditto + Doduo + /}             & 95.4             & 80.0             & 53.6          & 76.6 & 98.8          & 95.7          & 88.9             \\
% % \textbf{Ditto + Doduo + KBert}         & 95.5             & 72.0             & 72.7          & 75.4          & 96.9          & 95.6          & 69.4             \\
% \textbf{Ditto + EL}                    & 95.6             & 87.3             & 72.4          & 73.5          & 98.5          & 95.5          & 89.1             \\
% \textbf{Ditto + EL + /}                & 95.5             & 80.8             & 84.6          & 73.1          & 98.4          & \textbf{96.2} & 88.2             \\
% \textbf{Ditto + EL + KBert}            & 95.2             & 80.9             & 69.7          & 76.2          & 97.1          & 95.3          & 74.8             \\
% \textbf{Ditto + Sherlock + EL}         & 95.7             & 69.7             & 73.5          & 71.7          & 98.4          & \textbf{96.2} & 89.7             \\
% \textbf{Ditto + Sherlock + EL + /}     & \textbf{96.0}    & 75.4             & 61.5          & 75.5          & 98.3          & 95.2          & 89.8             \\
% \textbf{Ditto + Sherlock + EL + KBert} & 94.8             & 66.7             & 70.0          & 73.5          & 96.5          & 94.7          & 66.8             \\
% % \textbf{Ditto + Doduo + EL}            & 95.2             & 62.5             & 66.7          & 74.4          & \textbf{99.0} & 95.8          & \textbf{92.1}    \\
% % \textbf{Ditto + Doduo + EL + /}        & 95.3             & 62.5             & 54.5          & 75.5          & 98.8          & 95.4          & 89.0             \\
% % \textbf{Ditto + Doduo + EL + KBert}    & 94.1             & 69.2             & 71.7          & 76.2          & 95.8          & 94.9          & 56.7             \\ 
% \hline
% \end{tabular}
% }
% \caption{Results using different knowledge injection methods. All reported results are F1 scores. The first line represents the baseline model, Ditto without knowledge injection, and the subsequent rows represent the various knowledge injection and prompting methods combined with Ditto. "+Sherlock" indicates column semantic type prediction. "+EL" indicates knowledge injection with entity linking. "+/" represents prompting with a slash. "+KBert" represents prompting with additional position encoding, i.e., soft positions and visible matrix.}
% \label{tab:injection_results}
% \end{table*}





\subsubsection{Entity linking.}
The task of entity link aims to identify all entity mentions from a given knowledge base (KB) within the target text sequence. 
More specifically, 1) to identify the text span of each entity mention $m \in M$, and 2) map each mention to the entity set from an external KB $\mathcal{M}: M \rightarrow E$. 
Then additional knowledge from the KB is used to augment the input text records of the entity resolution model.
In this study, WikiData is used as the external KB since it covers a wide range of domains and is suitable for entity resolution over different datasets. 
Entity linking is accomplished using the state-of-the-art method proposed by~\citet{ayoola_refined_2022}, which uses a RoBERTa model jointly pre-trained over entity typing and entity description modeling objectives. The coarse entity type is used as the additional knowledge and injected into the model input.

% \subsubsection{Prompting for Knowledge Injection}

\subsubsection{Template-based prompting.}
After acquiring domain knowledge, we experiment using text-based templates to combine the knowledge with the initial text input. 
Two different characters are used to concatenate the original entity mention/column name, i.e. slash (``/'') and space. 
The slash symbol is chosen because it contains a semantic meaning equivalent to 'or' in general web text.
% First, the original entity mention/column name is concatenated with the injected knowledge text with simply a ``/'' token. The ``/'' is the same token as ``/'' in the text. 
% \yiren{Why a slash makes difference? maybe adding some citations ... }; 
% Second, simply using space to connect the original entity mention/column name and injected knowledge. 

% \begin{equation}
%     s(e_1, e_2) ::= [COL] attr_i [VAL] val_i [SEP] [COL] attr_j [val] val_j
% \end{equation}

\subsubsection{Prompting with additional position encoding.}
The domain knowledge injection modifies the initial input sequence in two dimensions, including the token position and token semantics. More specifically, the token from the initial input sequence has a different absolute position when the template-based prompting is concatenated. In addition, the injected knowledge for a specific entity mention is shared with the surrounding tokens in the same sequence. These two modifications might introduce erroneous knowledge and overload to the framework. 

To limit the erroneous knowledge and overload, we leverage additional position encoding, i.e., soft-position encoding and visible matrix, as an additional input of our backbone model, inspired by K\-Bert~\cite{liu_k-bert_2020}. Here, the knowledge-injected sequence is constructed into a tree structure, where the injected knowledge subsequences are considered branches. In this way, as shown in Figure~\ref{fig:framework}, the soft-position encoding assigns the initial tokens the same position encoding in the knowledge-injected sequence as those in the initial input sequence, and assigns the injected knowledge tokens the position in the branches. The visible matrix $V\in \mathcal{R}^{N \times N}$ is a binary matrix, where the injected knowledge token is only visible to itself and the corresponding entity mention or column name tokens, and set as 1, denoted as follows:
\begin{equation}
    V_{ij} = \begin{cases}
    1, & \text{if $token_i$ \text{and} $token_j$ co-occur}.\\
    0, & \text{otherwise}.
  \end{cases}
  \label{Eq:VM}
\end{equation}
where the co-occurrence of $token_i$ and $token_j$ indicates that the tokens co-occur in the initial input sequences or co-occur as head and tail in domain knowledge injection. Here, the head means the entity mention/column name, and the tail indicates the injected knowledge text. 
