


\subsection{Notation of Entity Resolution Task}
The formulation of the entity resolution task in this paper is based on the current work by~\citet{li_deep_2020}. The input of the entity resolution task consists of a set $M \subseteq D_1 \times D_2$, where $D_1$ and $D_2$ are two sets of data entry collections that contain duplicated entries.  
For each data entry, $e \in {(col_i, val_i)}_{1 \leq i \leq N}$ where $N$ is the number of columns, and $(e_1, e_2) \in M$. The task discussed in this paper focuses on: for each data entry pair $(e_1, e_2) \in M$, determine whether $e_1$ and $e_2$ refer to the same data entity.
% \yiren{TODO}

\subsection{Model Overview}
This study introduces a new framework for entity resolution: KAER (See Figure~\ref{fig:framework}). 
% \bl{in abstract + introduction}
% KAER is based on a transformer; 
% Three prompting types
KAER uses PLMs for entity resolution and contains three modules for knowledge augmentation: a) column semantic type augmentation, b) entity semantic type augmentation, and c) three options of prompting types. 
The following sections first describe the PLMs for entity resolution and then each module.


\subsubsection{Pre-trained language model for entity resolution.}
Following the work by~\citet{li_deep_2020}, KAER uses RoBERTa as the backbone model. For each data entry pair, $(e_1, e_2)$, 
 the text context of column names and values of $e_1$ and $e_2$ are serialized and concatenated as the input for PLMs. The [CLS] token position is used to classify whether $e_1$ and $e_2$ refer to the same entity. 
The loss for optimizing the classification objective is:
\begin{equation}
    \ell = - log\ p(y | s(e_1, e_2))
\end{equation}

where y denotes whether $e_1$ and $e_2$ refer to the same entity, and $s(\cdot,\cdot)$ denotes the serialization and transformation of entity pairs with knowledge injection and prompting methods. 
\begin{equation}
\Scale[0.8]{
    s(e_i, e_j) ::=  \text{[CLS]}\ serialize(e_i)\ \text{[SEP]}\ serialize(e_j)\ \text{[SEP]}} 
\end{equation}
where $serialize(\cdot)$ serializes each data entry.
\begin{equation}\Scale[0.8]{
\begin{split}
serialize(e_i) ::= & \text{[COL]}\ f(col_1,pt)\ \text{[VAL]}\ g(val_1,pt)\ ...\\
            & \text{[COL]}\ f(col_N,pt)\ \text{[VAL]}\ g(val_N,pt)   
\end{split}}
\end{equation}
where $f(col_i, pt)$ denotes the semantic column type injection with prompting method $pt$, and $g(val_i, pt)$ denotes the entity linking injection with prompting method $pt$. The following sections describe  column semantic type injection, entity linking, and three prompting methods.

% \subsubsection{Knowledge Augmentation}

\subsubsection{Column semantic type augmentation.}
Column type prediction aims to predict the semantic type of each column by considering the intra-column context, so that fine-grained semantic information can be augmented to the framework. For example, ``name'' as a column name can provide more semantic information if injected with the semantic type ``song''.  %of which the output is $f(attr_i,p) = p(attr_i, t)$, where t is the predicted semantic type.
% two methods for column type prediction: Sherlock \cite{hulsebos_sherlock_2019} and Doduo \cite{suhara_annotating_2022}. 
Here, KAER uses Sherlock~\cite{hulsebos_sherlock_2019} for column semantic type prediction. In detail, \textit{Sherlock} is a multi-input deep neural network pre-trained on 686,765 data columns retrieved from the 
VizNet corpus by matching 78 semantic data types from DBpedia to column headers \cite{hulsebos_sherlock_2019}. %Four categories of features are generated by Sherlock and fed into the neural network, including global statistics, aggregated character distributions, pre-trained word embeddings, and self-trained paragraph vectors. They train subnetworks for aggregated character distributions, pre-trained word embeddings, and paragraph vectors. Then, they concatenate the weights of the three output layers with global statistics features to form the input of Sherlock.

% \textit{Doduo} is a multi-task learning framework based on PLMs~\cite{suhara_annotating_2022}. Doduo serializes the entire table into a sequence of tokens to make it fit for Transformer-based architecture. Consider a table T consisting of a set of attributes, i.e., $T= (c_1, c_2,... c_n)$, and the list of data values stored at the column $c_i$ is denoted as $val(T.c_i)$. For each value v, it can be of string type and split into a sequence of input tokens to PLMs. The serialization strategy of Doduo is simply concatenating column values to make a sequence of tokens and feed that sequence as input to the model \cite{suhara_annotating_2022}. 

% \begin{equation}
%     serialize_{single}(c_i) ::= [CLS]\ v_1\ ...\ v_m\ [SEP], \\ 
% \end{equation}

% where [CLS] and [SEP] are special tokens to mark the beginning and end of a sequence, and ${v_1 ... v_m} \in val(T.c_i)$.


%%%%%%%%%%%%%%%%%%%%%%%%table%%%%%%%%%%%%%%
\input{04table1.tex}

\subsubsection{Entity semantic type augmentation.}
Entity semantic type augmentation leverages the entity linking method to identify all entity mentions from a given knowledge base (KB) within the target text sequence. 
More specifically, 1) to identify the text span of each entity mention $m \in M$, and 2) map each mention to the entity set from an external KB $\mathcal{M}: M \rightarrow E$. 
Thus, additional knowledge from the KB is used to augment the input text records.
In this study, WikiData is used as the external KB since it covers a wide range of domains and is suitable for entity resolution over different datasets. 
KAER uses the state-of-the-art method proposed by~\citet{ayoola_refined_2022}, which uses a RoBERTa model jointly pre-trained over entity typing and entity description modeling objectives to inject coarse entity types. %The coarse entity type is used as the additional knowledge and injected into the model input.

% \subsubsection{Prompting for Knowledge Injection}
\subsubsection{Prompting methods}
\paragraph{Template-based prompting.}
After acquiring domain knowledge, we experiment using text-based templates to combine the knowledge with the initial text input. 
Two different characters are used to concatenate the original entity mention/column name, i.e. slash (``/'') and space. 
The slash symbol is chosen because it contains a semantic meaning equivalent to 'or' in general web text.
% First, the original entity mention/column name is concatenated with the injected knowledge text with simply a ``/'' token. The ``/'' is the same token as ``/'' in the text. 
% \yiren{Why a slash makes difference? maybe adding some citations ... }; 
% Second, simply using space to connect the original entity mention/column name and injected knowledge. 

% \begin{equation}
%     s(e_1, e_2) ::= [COL] attr_i [VAL] val_i [SEP] [COL] attr_j [val] val_j
% \end{equation}

\paragraph{Prompting with constrained tuning}
Inspired by hard-soft prompt hybrid tuning~\cite{han_2021},  prompting with constrained tuning is used to generate the token embedding by controlling the visible area of the augmented knowledge only to its corresponding entity mentions or column names. This type of domain knowledge injection modifies the initial input sequence in two dimensions, including the token position and token semantics. More specifically, the token from the initial input sequence has a different absolute position when the template-based prompting is concatenated. In addition, the injected knowledge for a specific entity mention is shared with the surrounding tokens in the same sequence. These two modifications might introduce erroneous knowledge and overload to the framework.

To limit the erroneous knowledge and overload, we leverage constrained tuning, i.e., soft-position encoding and visible matrix, as an additional input of our backbone model, inspired by K\-Bert~\cite{liu_k-bert_2020}. Here, the knowledge-injected sequence is constructed into a tree structure, where the injected knowledge subsequences are considered branches. In this way, as shown in Figure~\ref{fig:framework}, the soft-position encoding assigns the initial tokens the same position encoding in the knowledge-injected sequence as those in the initial input sequence, and assigns the injected knowledge tokens the position in the branches. The visible matrix $V\in \mathcal{R}^{N \times N}$ is a binary matrix, where the injected knowledge token is only visible to itself and the corresponding entity mention or column name tokens, and set as 1, denoted as follows:
\begin{equation}
    V_{ij} = \begin{cases}
    1, & \text{if $token_i$ \text{and} $token_j$ co-occur}.\\
    0, & \text{otherwise}.
  \end{cases}
  \label{Eq:VM}
\end{equation}
where the co-occurrence of $token_i$ and $token_j$ indicates that the tokens co-occur in the initial input sequences or co-occur as head and tail in domain knowledge injection. Here, the head means the entity mention/column name, and the tail indicates the injected knowledge text. 
