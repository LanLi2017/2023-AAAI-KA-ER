\begin{abstract}

Pre-trained language models have been widely applied and proven effective in various Natural Language Processing (NLP) tasks. Prior studies have also examined the possibility of using Pretrained Language Models (PLMs) for multiple data cleaning tasks, e.g., entity resolution (or entity matching). However, in reality, there are various challenges for the entity resolution tasks on real-world datasets, including heterogeneous formats, excessive length of data, and complicity of the semantic meaning within data. 
% 1. data from heterogeneous sources are in different format resulting in a messy input, 2. long sentences or descriptions that make it difficult to do the entity matching by simply calculating the distances between records, and 3. the semantic of data is not expressive enough to be understood by models without leveraging domain knowledge or referring to external entities.
% textual data contains references to external entities that are not explicitly mentioned within the data itself. 
In this study, we demonstrate how knowledge augmentation can be incorporated into PLMs without having to finetune the model at a high computational and labeling cost. 

\end{abstract}
